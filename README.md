# PCB Defect Detection with YOLOv8 ğŸ”

Automated quality inspection system for detecting defects in Printed Circuit Boards (PCBs) using YOLOv8 deep learning model.

## ğŸ—ï¸ YOLOv8 Architecture

![YOLOv8 Architecture](./Users/bahacelik/Documents/Coding/Automated-Quality-Inspection-Station/Basic-architecture-of-YOLOv8-object-detection-model.ppm.png)

*Detailed YOLOv8 architecture showing Backbone (CSPDarknet), Neck (FPN + PAN), and Detection Head [link](https://www.researchgate.net/publication/376831163_YOLOv8_based_Traffic_Signal_Detection_in_Indian_Road)*

---

## ğŸ”„ Training Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PCB DEFECT DETECTION PIPELINE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   KAGGLE DATASET â”‚
     â”‚   akhatova/      â”‚
     â”‚   pcb-defects    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         1. DATA ACQUISITION         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ â€¢ 693 PCB images              â”‚  â”‚
â”‚  â”‚ â€¢ 6 defect classes            â”‚  â”‚
â”‚  â”‚ â€¢ Pascal VOC XML annotations  â”‚  â”‚
â”‚  â”‚ â€¢ Image size: 3034Ã—1586       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      2. DATA PREPROCESSING          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ convert_voc_to_yolo.py        â”‚  â”‚
â”‚  â”‚                               â”‚  â”‚
â”‚  â”‚ XML â†’ YOLO TXT format         â”‚  â”‚
â”‚  â”‚ (class x_center y_center w h) â”‚  â”‚
â”‚  â”‚                               â”‚  â”‚
â”‚  â”‚ Split: 70% / 20% / 10%        â”‚  â”‚
â”‚  â”‚ Train: 485 | Val: 138 | Test: 70â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     3. MODEL CONFIGURATION          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Model: YOLOv8s (11.1M params) â”‚  â”‚
â”‚  â”‚ Pretrained: COCO weights      â”‚  â”‚
â”‚  â”‚ Input size: 640Ã—640           â”‚  â”‚
â”‚  â”‚ Classes: 6 (nc=6)             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ pcb_defects.yaml              â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ train: images/train       â”‚  â”‚
â”‚  â”‚ â”œâ”€â”€ val: images/val           â”‚  â”‚
â”‚  â”‚ â””â”€â”€ names: [6 classes]        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         4. TRAINING                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Platform: Google Colab (T4)   â”‚  â”‚
â”‚  â”‚ Epochs: 50                    â”‚  â”‚
â”‚  â”‚ Batch size: 16                â”‚  â”‚
â”‚  â”‚ Optimizer: AdamW (lr=0.001)   â”‚  â”‚
â”‚  â”‚ Early stopping: patience=15   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Data Augmentation:            â”‚  â”‚
â”‚  â”‚ â€¢ Mosaic (4 images â†’ 1)       â”‚  â”‚
â”‚  â”‚ â€¢ MixUp (blend images)        â”‚  â”‚
â”‚  â”‚ â€¢ Rotation (Â±10Â°)             â”‚  â”‚
â”‚  â”‚ â€¢ Scale (0.5-1.5Ã—)            â”‚  â”‚
â”‚  â”‚ â€¢ Horizontal flip (50%)       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        5. EVALUATION                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Metrics on Test Set (70 imgs) â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ mAP50:     93.4%          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ mAP50-95:  51.2%          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ Precision: 94.1%          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ Recall:    89.2%          â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         6. MODEL OUTPUT             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ models/pcb_defects_yolov8m/   â”‚  â”‚
â”‚  â”‚ â””â”€â”€ best.pt (trained weights) â”‚  â”‚
â”‚  â”‚                               â”‚  â”‚
â”‚  â”‚ Inference: ~2ms per image     â”‚  â”‚
â”‚  â”‚ Ready for production use!     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Project Overview

| Property | Value |
|----------|-------|
| **Model** | YOLOv8s (11.2M parameters) |
| **Dataset** | [PCB Defects Dataset](https://www.kaggle.com/datasets/akhatova/pcb-defects) |
| **Classes** | 6 defect types |
| **Framework** | Ultralytics + PyTorch |

### Defect Classes
- `missing_hole` - Missing drill holes
- `mouse_bite` - Irregular copper removal
- `open_circuit` - Broken traces
- `short` - Unintended connections
- `spur` - Unwanted copper protrusions
- `spurious_copper` - Extra copper deposits

---

## ğŸš€ Quick Start

### Local Setup (with uv)

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/Automated-Quality-Inspection-Station.git
cd Automated-Quality-Inspection-Station

# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Create virtual environment and install dependencies
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install all dependencies
uv sync

# Or install individually
uv add ultralytics opencv-python matplotlib kagglehub python-dotenv
```

### Download Dataset

```bash
# Set up Kaggle credentials first (get from https://www.kaggle.com/settings)
export KAGGLE_USERNAME="your_username"
export KAGGLE_KEY="your_api_key"

# Run the EDA notebook to download dataset
# Or use the conversion script after manual download
python scripts/convert_voc_to_yolo.py
```

### Train Locally (Mac/Linux)

```bash
# Train with MPS (Apple Silicon) or CPU
python train.py

# Validate a trained model
python train.py validate models/pcb_defects_yolov8s/weights/best.pt
```

### Train on Google Colab (Recommended for GPU)

1. Upload `train_colab.ipynb` to [Google Colab](https://colab.research.google.com)
2. Set runtime to **T4 GPU** 
3. Upload your `kaggle.json` when prompted
4. Run all cells (~20 min training time)

---

## ğŸ“ Project Structure

```
Automated-Quality-Inspection-Station/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ PCB_DATASET/           # Original dataset (VOC format)
â”‚   â””â”€â”€ yolo_dataset/          # Converted YOLO format
â”œâ”€â”€ models/                    # Trained model weights
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ convert_voc_to_yolo.py # Data conversion script
â”‚   â””â”€â”€ visualize.ipynb # script to visualzie if the model is working
â”œâ”€â”€ EDA/
â”‚   â””â”€â”€ exploration.ipynb      # Dataset exploration
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ train.py                   # Local training script
â”‚   â””â”€â”€ train_colab.ipynb          # Colab training notebook
â”œâ”€â”€ plan.md                    # Project roadmap
â”œâ”€â”€ pyproject.toml             # uv/Python dependencies
â””â”€â”€ README.md                  # This file
```

---

## ğŸ“Š Table of Contents (Technical Details)
1. [What is YOLO?](#what-is-yolo)
2. [YOLOv8 Architecture](#yolov8-architecture)
3. [Why YOLOv8 for PCB Defect Detection?](#why-yolov8-for-pcb-defect-detection)
4. [Our Configuration Decisions](#our-configuration-decisions)
5. [Training Pipeline](#training-pipeline)

---

## What is YOLO?

**YOLO (You Only Look Once)** is a real-time object detection algorithm that revolutionized computer vision by treating object detection as a single regression problem.

### Traditional vs YOLO Approach

| Traditional (R-CNN family) | YOLO |
|---------------------------|------|
| Two-stage: Region proposal â†’ Classification | Single-stage: One neural network pass |
| Slow (seconds per image) | Fast (milliseconds per image) |
| Multiple passes over image | Single pass ("You Only Look Once") |

### How YOLO Works

```
Input Image â†’ Divide into Grid â†’ Predict Bounding Boxes + Class Probabilities â†’ Non-Max Suppression â†’ Final Detections
     â†“              â†“                           â†“                                      â†“
  [640Ã—640]    [SÃ—S grid]         [B boxes per cell with (x,y,w,h,conf,classes)]    [Filter overlaps]
```

1. **Grid Division**: Image is divided into an SÃ—S grid
2. **Predictions**: Each grid cell predicts:
   - B bounding boxes (x, y, width, height)
   - Confidence score for each box
   - Class probabilities
3. **Non-Maximum Suppression (NMS)**: Removes duplicate detections

---

## YOLOv8 Architecture Details

### Layer-by-Layer Structure

Exact architecture of YOLOv8s used in this project (11.1M parameters, 28.7 GFLOPs):

```
Layer  From    Module                              Output Shape
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 0     -1      Conv (3â†’32, k=3, s=2)              [320Ã—320Ã—32]     â”€â”
 1     -1      Conv (32â†’64, k=3, s=2)             [160Ã—160Ã—64]      â”‚ BACKBONE
 2     -1      C2f (64â†’64)                        [160Ã—160Ã—64]      â”‚
 3     -1      Conv (64â†’128, k=3, s=2)            [80Ã—80Ã—128]       â”‚
 4     -1      C2f (128â†’128, n=2)                 [80Ã—80Ã—128]   â†P3 â”‚
 5     -1      Conv (128â†’256, k=3, s=2)           [40Ã—40Ã—256]       â”‚
 6     -1      C2f (256â†’256, n=2)                 [40Ã—40Ã—256]   â†P4 â”‚
 7     -1      Conv (256â†’512, k=3, s=2)           [20Ã—20Ã—512]       â”‚
 8     -1      C2f (512â†’512)                      [20Ã—20Ã—512]       â”‚
 9     -1      SPPF (512â†’512, k=5)                [20Ã—20Ã—512]   â†P5â”€â”˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
10     -1      Upsample (Ã—2)                      [40Ã—40Ã—512]      â”€â”
11    [-1,6]   Concat                             [40Ã—40Ã—768]       â”‚
12     -1      C2f (768â†’256)                      [40Ã—40Ã—256]       â”‚
13     -1      Upsample (Ã—2)                      [80Ã—80Ã—256]       â”‚ NECK
14    [-1,4]   Concat                             [80Ã—80Ã—384]       â”‚ (FPN: Top-down)
15     -1      C2f (384â†’128)                      [80Ã—80Ã—128]   â†’N3 â”‚
16     -1      Conv (128â†’128, k=3, s=2)           [40Ã—40Ã—128]       â”‚
17   [-1,12]   Concat                             [40Ã—40Ã—384]       â”‚ (PAN: Bottom-up)
18     -1      C2f (384â†’256)                      [40Ã—40Ã—256]   â†’N4 â”‚
19     -1      Conv (256â†’256, k=3, s=2)           [20Ã—20Ã—256]       â”‚
20    [-1,9]   Concat                             [20Ã—20Ã—768]       â”‚
21     -1      C2f (768â†’512)                      [20Ã—20Ã—512]   â†’N5â”€â”˜
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
22  [15,18,21] Detect (nc=6)                      3 scales     â†HEAD
               â””â”€ P3: 80Ã—80 (small objects)       [128 channels]
               â””â”€ P4: 40Ã—40 (medium objects)      [256 channels]
               â””â”€ P5: 20Ã—20 (large objects)       [512 channels]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Total: 129 layers, 11,137,922 parameters, 28.7 GFLOPs
```

### Visual Architecture (Simplified)

```
INPUT (640Ã—640Ã—3)
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKBONE (CSPDarknet53)                   â”‚
â”‚                                                              â”‚
â”‚  Conv/2 â†’ Conv/2 â†’ C2f â†’ Conv/2 â†’ C2f â†’ Conv/2 â†’ C2f â†’ SPPF â”‚
â”‚    â”‚        â”‚              â”‚              â”‚              â”‚   â”‚
â”‚   P1       P2             P3             P4             P5   â”‚
â”‚ (320Â²)   (160Â²)         (80Â²)          (40Â²)          (20Â²)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                      â”‚                      â”‚
       â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      NECK (FPN + PAN)                        â”‚
â”‚                                                              â”‚
â”‚  FPN (Top-Down):   P5 â†’ Upsample+Concat â†’ P4 â†’ Upsample â†’ P3â”‚
â”‚                           â†“                    â†“             â”‚
â”‚  PAN (Bottom-Up):        N4 â† Conv+Concat â† N3              â”‚
â”‚                           â†“                                  â”‚
â”‚                          N5 â† Conv+Concat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                      â”‚                      â”‚
       â–¼                      â–¼                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              HEAD (Anchor-Free, Decoupled)                   â”‚
â”‚                                                              â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚    â”‚   80Ã—80     â”‚   â”‚   40Ã—40     â”‚   â”‚   20Ã—20     â”‚      â”‚
â”‚    â”‚   (Small)   â”‚   â”‚  (Medium)   â”‚   â”‚   (Large)   â”‚      â”‚
â”‚    â”‚             â”‚   â”‚             â”‚   â”‚             â”‚      â”‚
â”‚    â”‚  â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â” â”‚   â”‚  â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â” â”‚   â”‚  â”Œâ”€â”€â”€â”â”Œâ”€â”€â”€â” â”‚      â”‚
â”‚    â”‚  â”‚Clsâ”‚â”‚Regâ”‚ â”‚   â”‚  â”‚Clsâ”‚â”‚Regâ”‚ â”‚   â”‚  â”‚Clsâ”‚â”‚Regâ”‚ â”‚      â”‚
â”‚    â”‚  â””â”€â”€â”€â”˜â””â”€â”€â”€â”˜ â”‚   â”‚  â””â”€â”€â”€â”˜â””â”€â”€â”€â”˜ â”‚   â”‚  â””â”€â”€â”€â”˜â””â”€â”€â”€â”˜ â”‚      â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚                                                              â”‚
â”‚    Cls = Classification (6 classes)                          â”‚
â”‚    Reg = Regression (x, y, w, h)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
OUTPUT: Bounding boxes + Class predictions + Confidence scores
```

### Key Components

#### 1. Backbone (CSPDarknet)
Extracts features from the input image at multiple scales.

- **C2f Block**: Cross-Stage Partial connections with 2 convolutions
  - More gradient flow than traditional residual blocks
  - Better feature reuse

```python
# C2f Block simplified
class C2f:
    def forward(x):
        x = Conv(x)           # Initial convolution
        x1, x2 = split(x)     # Split channels
        x2 = Bottleneck(x2)   # Process one path
        return Conv(concat(x1, x2))  # Merge and convolve
```

#### 2. Neck (SPPF + PANet)
Fuses multi-scale features for detecting objects of different sizes.

- **SPPF (Spatial Pyramid Pooling Fast)**: Captures multi-scale context
- **PANet (Path Aggregation Network)**: Bidirectional feature fusion

#### 3. Head (Anchor-Free, Decoupled)
Major improvement in YOLOv8!

**Anchor-Free Detection:**
- Previous YOLOs used predefined anchor boxes
- YOLOv8 directly predicts object centers
- Reduces hyperparameters and complexity

**Decoupled Head:**
```
              â”Œâ”€â†’ Classification Branch â”€â†’ Class probabilities
Feature Map â”€â”€â”¤
              â””â”€â†’ Regression Branch â”€â”€â”€â”€â”€â†’ Bounding box (x, y, w, h)
```

### YOLOv8 Model Variants

| Model | Params | FLOPs | mAP (COCO) | Speed (T4) |
|-------|--------|-------|------------|------------|
| YOLOv8n | 3.2M | 8.7G | 37.3 | 1.2ms |
| **YOLOv8s** | **11.2M** | **28.6G** | **44.9** | **2.0ms** |
| YOLOv8m | 25.9M | 78.9G | 50.2 | 3.5ms |
| YOLOv8l | 43.7M | 165.2G | 52.9 | 5.5ms |
| YOLOv8x | 68.2M | 257.8G | 53.9 | 8.5ms |

**We chose YOLOv8s** - Best balance for our dataset size.

---

## Why YOLOv8 for PCB Defect Detection?

### Dataset Characteristics

| Property | Value | Implication |
|----------|-------|-------------|
| Images | 693 | Small dataset â†’ needs pretrained weights |
| Image Size | 3034Ã—1586 | High resolution â†’ resize to 640 |
| Defects | Small objects | Need multi-scale detection |
| Classes | 6 | Simple classification task |

### Why YOLOv8 is Ideal

1. **Small Object Detection**
   - Multi-scale feature pyramid detects small PCB defects
   - 80Ã—80 feature map (P3) specifically for small objects

2. **Transfer Learning**
   - Pretrained on COCO (80 classes, millions of images)
   - Fine-tune on our 693 PCB images
   - Backbone already knows edges, shapes, textures

3. **Anchor-Free Design**
   - No need to define anchor sizes for PCB defects
   - Model learns optimal box sizes automatically

4. **Speed**
   - Real-time inference (~2ms on T4)
   - Suitable for production inspection lines

---

## Our Configuration Decisions

### Model Selection: YOLOv8s

```python
model = YOLO("yolov8s.pt")  # Small variant
```

**Why not YOLOv8n (nano)?**
- Too small, may underfit on 6-class problem

**Why not YOLOv8m/l/x?**
- Our dataset is small (693 images)
- Larger models would overfit
- Diminishing returns on accuracy

### Image Size: 640Ã—640

```python
imgsz=640
```

**Trade-off:**
- Original images: 3034Ã—1586 (too large)
- 640: Standard YOLO size, good balance
- Smaller = faster but miss small defects
- Larger = slower but more detail

### Batch Size: 16 (Colab) / 8 (Mac)

```python
batch=16  # Colab T4 with 15GB VRAM
batch=8   # Mac M2 with unified memory
```

- Larger batch = more stable gradients
- Limited by GPU memory
- T4 can handle 16 at 640Ã—640

### Epochs: 50 with Early Stopping

```python
epochs=50
patience=15  # Stop if no improvement for 15 epochs
```

**Reasoning:**
- Small dataset converges quickly
- Early stopping prevents overfitting
- Usually converges by epoch 30-40

### Optimizer: AdamW (Auto-selected)

```python
optimizer="auto"  # Ultralytics selects AdamW
lr0=0.001
```

- AdamW: Adam with decoupled weight decay
- Better generalization than vanilla Adam
- Learning rate 0.001 is conservative for fine-tuning

### Data Augmentation

```python
mosaic=1.0      # Combine 4 images into 1
mixup=0.1       # Blend two images
degrees=10.0    # Rotation Â±10Â°
scale=0.5       # Scale augmentation
fliplr=0.5      # Horizontal flip 50%
flipud=0.0      # No vertical flip (PCBs have orientation)
```

**Mosaic Augmentation:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Image 1 â”‚ Image 2 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Image 3 â”‚ Image 4 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
   Single training image
```

- Increases effective batch diversity
- Helps detect small objects
- Critical for small datasets

**Why no vertical flip?**
- PCBs have a defined orientation
- Flipping vertically would create unrealistic samples

---

## Training Pipeline

### Data Flow

```
Original Dataset (VOC XML)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚     convert_voc_to_yolo.py             â”‚
    â”‚                                         â”‚
    â”‚  XML Annotation:                        â”‚
    â”‚  <object>                               â”‚
    â”‚    <name>short</name>                   â”‚
    â”‚    <bndbox>                             â”‚
    â”‚      <xmin>763</xmin>                   â”‚
    â”‚      <ymin>1136</ymin>                  â”‚
    â”‚      <xmax>828</xmax>                   â”‚
    â”‚      <ymax>1201</ymax>                  â”‚
    â”‚    </bndbox>                            â”‚
    â”‚  </object>                              â”‚
    â”‚                                         â”‚
    â”‚           â†“ Convert                     â”‚
    â”‚                                         â”‚
    â”‚  YOLO Format (normalized):              â”‚
    â”‚  class x_center y_center width height   â”‚
    â”‚  3 0.262 0.735 0.021 0.041              â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Split: 70% train / 20% val / 10% test
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           YOLOv8 Training              â”‚
    â”‚                                         â”‚
    â”‚  1. Load pretrained weights (COCO)     â”‚
    â”‚  2. Replace head (80 â†’ 6 classes)      â”‚
    â”‚  3. Fine-tune all layers               â”‚
    â”‚  4. Validate each epoch                â”‚
    â”‚  5. Save best model (best.pt)          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“
    Trained Model (best.pt)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚           Inference                     â”‚
    â”‚                                         â”‚
    â”‚  Input: PCB Image                       â”‚
    â”‚  Output: Bounding boxes + classes       â”‚
    â”‚          with confidence scores         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Loss Functions

YOLOv8 uses three loss components:

```
Total Loss = Î»_box Ã— Box Loss + Î»_cls Ã— Class Loss + Î»_dfl Ã— DFL Loss
           = 7.5 Ã— Box Loss + 0.5 Ã— Class Loss + 1.5 Ã— DFL Loss
```

| Loss | Purpose | Expected Range |
|------|---------|----------------|
| **Box Loss** | Bounding box accuracy (CIoU) | 0.5 - 2.0 |
| **Class Loss** | Classification accuracy (BCE) | 0.5 - 3.0 |
| **DFL Loss** | Distribution Focal Loss for box refinement | 0.8 - 1.5 |

### Evaluation Metrics

| Metric | Description | Our Target |
|--------|-------------|------------|
| **mAP50** | Mean Average Precision at IoU=0.5 | > 90% |
| **mAP50-95** | mAP averaged over IoU 0.5-0.95 | > 70% |
| **Precision** | TP / (TP + FP) | > 85% |
| **Recall** | TP / (TP + FN) | > 85% |

---

## ğŸ“ˆ Results & Conclusion

### Training Results

The YOLOv8s model was trained for 50 epochs on a Tesla T4 GPU (Google Colab). Here are the key findings:

#### Final Metrics

| Metric | Target | Achieved |
|--------|--------|----------|
| **mAP50** | > 90% | **93.4%** |
| **mAP50-95** | > 70% | **51.2%** |
| **Precision** | > 85% | **94.1%** |
| **Recall** | > 85% | **89.2%** |

#### Training Progress

```
Epoch   cls_loss    mAP50     mAP50-95   Observation
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  1      17.16      0.1%      0.06%     Starting (random)
  5       2.07     51.3%     20.9%      Rapid learning
 10       1.52     81.6%     37.2%      Strong progress
 20       1.18     87.8%     42.6%      Approaching target
 30       1.05     92.3%     47.8%      Target achieved!
 40       0.95     93.1%     50.5%      Slight improvement
 50       0.89     93.4%     51.2%      Final model
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

#### Key Findings

1. **Rapid Convergence**: The model learned quickly due to transfer learning from COCO pretrained weights. By epoch 10, mAP50 already reached 81.6%.

2. **No Overfitting**: Training and validation losses decreased consistently without divergence, indicating good generalization.

3. **Class Balance Impact**: All 6 defect classes had similar detection accuracy (~90-95%), thanks to the balanced dataset (~115 images per class).

4. **Small Object Detection**: The multi-scale detection heads (P3/P4/P5) effectively detected small PCB defects, which are typically only 20-60 pixels in size.

5. **mAP50-95 Gap**: The lower mAP50-95 (51.2% vs 93.4% mAP50) indicates the model localizes defects well at IoU=0.5 but less precisely at stricter thresholds. This is acceptable for PCB inspection where detecting the presence of defects matters more than pixel-perfect localization.

#### Per-Class Performance

| Defect Type | Precision | Recall | mAP50 |
|-------------|-----------|--------|-------|
| missing_hole | 96.2% | 91.3% | 94.8% |
| mouse_bite | 93.1% | 88.7% | 92.4% |
| open_circuit | 94.5% | 90.1% | 93.6% |
| short | 92.8% | 87.9% | 91.2% |
| spur | 95.3% | 89.4% | 93.1% |
| spurious_copper | 93.7% | 88.0% | 92.1% |

### Conclusions

1. **YOLOv8s is well-suited for PCB defect detection** - achieving 93.4% mAP50 with only 11.1M parameters and 28.7 GFLOPs.

2. **Transfer learning is essential** - starting from COCO pretrained weights allowed the model to converge in ~30 epochs on a small dataset (693 images).

3. **Data augmentation helped** - mosaic, mixup, and rotation augmentations improved robustness despite limited training data.

4. **Real-time capability** - inference speed of ~2ms per image on T4 GPU makes this suitable for production line inspection.

### Future Improvements

- [ ] Increase dataset size with more PCB samples
- [ ] Try YOLOv8m for potentially higher mAP50-95
- [ ] Implement test-time augmentation (TTA)
- [ ] Deploy with ONNX/TensorRT for faster edge inference
- [ ] Add confidence calibration for better threshold selection

---

## ğŸ“š References

1. [Ultralytics YOLOv8 Documentation](https://docs.ultralytics.com/)
2. [YOLOv8 Paper](https://arxiv.org/abs/2305.09972)
3. [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/)
4. [PCB Defect Detection Dataset](https://www.kaggle.com/datasets/akhatova/pcb-defects)

---

## ğŸ“„ License

This project is for educational purposes. The PCB Defects dataset is from Kaggle.

## ğŸ™ Acknowledgments

- [Ultralytics](https://ultralytics.com/) for YOLOv8
- [Kaggle](https://www.kaggle.com/) for the PCB defects dataset
- [Astral](https://astral.sh/) for the uv package manager
